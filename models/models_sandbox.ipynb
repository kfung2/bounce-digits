{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/kelvinfung/Documents/bounce-digits\")\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, F, H, W = 50, 1, 5, 64, 64\n",
    "vid_sample = torch.randn(N, C, F, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.FloatTensor([[5, 1], [0, 3]])\n",
    "sample2 = torch.FloatTensor([[3, 4], [10, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2500)\n",
      "tensor(37.2500)\n"
     ]
    }
   ],
   "source": [
    "print(nn.L1Loss()(sample1, sample2))\n",
    "print(nn.MSELoss()(sample1, sample2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without stride; same output HxW\n",
    "conv3d = nn.Conv3d(1, 5,\n",
    "                   stride=(1, 1, 1),\n",
    "                   kernel_size=(3,3,3), padding=(1,1,1))\n",
    "\n",
    "conv3d(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With stride; Downsample by halving height and width\n",
    "conv3d = nn.Conv3d(1, 5,\n",
    "                   stride=(1, 2, 2),\n",
    "                   kernel_size=(3,3,3), padding=(1,1,1))\n",
    "\n",
    "conv3d(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranpose; Same height and width\n",
    "conv3dtranspose = nn.ConvTranspose3d(1, 5,\n",
    "                           stride=(1,1,1),kernel_size=(3,3,3), \n",
    "                           padding=(1,1,1))\n",
    "\n",
    "conv3dtranspose(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranpose upsample; Doubles the height and width\n",
    "conv3dtranspose = nn.ConvTranspose3d(1, 5,\n",
    "                           stride=(1,2,2),kernel_size=(3,3,3), \n",
    "                           padding=(1,1,1), output_padding=(0,1,1))\n",
    "\n",
    "conv3dtranspose(vid_sample).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample batch of context frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, F, H, W = 50, 1, 5, 64, 64\n",
    "sample_batch = torch.randn(N, C, F, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = torch.randint_like(vid_sample, low=-5, high=15)\n",
    "plt.hist(sample_batch.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FutureDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'nframes_pred': 5,\n",
    "#     'nframes_in' : 5,\n",
    "#     'batch_norm' : False,\n",
    "#     'w_norm' : True,\n",
    "#     'loss' : 'wgan_gp',\n",
    "#     'd_gdrop' : False,\n",
    "#     'padding' : 'zero',\n",
    "#     'lrelu' : True,\n",
    "#     'd_sigmoid' : False,\n",
    "#     'nz' : 512,            # dim of input noise vector z    \n",
    "#     'nc' : 1,              # number of channels\n",
    "#     'ndf' : 512,           # discriminator first layer's feature dim\n",
    "#     'd_cond' : True\n",
    "# }\n",
    "\n",
    "# dis = Discriminator(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "######## Refer to the implementation given at 'https://github.com/edenton/drnet-py' for indepth understanding ###########\n",
    "\n",
    "class MovingMNIST(object):\n",
    "    \n",
    "    \"\"\"Data Handler that creates Bouncing MNIST dataset on the fly.\"\"\"\n",
    "\n",
    "    def __init__(self, train, data_root, seq_len=20, num_digits=2, image_size=64):\n",
    "        path = data_root\n",
    "        self.seq_len = seq_len\n",
    "        self.num_digits = num_digits  \n",
    "        self.image_size = image_size \n",
    "        self.step_length = 0.1\n",
    "        self.digit_size = 32\n",
    "        self.seed_is_set = False # multi threaded loading\n",
    "\n",
    "        self.data = datasets.MNIST(\n",
    "            path,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.Resize(self.digit_size),\n",
    "                 transforms.ToTensor()]))\n",
    "\n",
    "        self.N = len(self.data) \n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        if not self.seed_is_set:\n",
    "            self.seed_is_set = True\n",
    "            np.random.seed(seed)\n",
    "          \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.set_seed(index)\n",
    "        image_size = self.image_size\n",
    "        digit_size = self.digit_size\n",
    "        x = np.zeros((self.seq_len,\n",
    "                      image_size, \n",
    "                      image_size, \n",
    "                      3),\n",
    "                    dtype=np.float32)\n",
    "        for n in range(self.num_digits):\n",
    "            idx = np.random.randint(self.N)\n",
    "            digit, _ = self.data[idx]\n",
    "\n",
    "            sx = np.random.randint(image_size-digit_size)\n",
    "            sy = np.random.randint(image_size-digit_size)\n",
    "            dx = np.random.randint(-4, 4)\n",
    "            dy = np.random.randint(-4, 4)\n",
    "            for t in range(self.seq_len):\n",
    "                if sy < 0:\n",
    "                    sy = 0 \n",
    "                    dy = -dy\n",
    "                elif sy >= image_size-32:\n",
    "                    sy = image_size-32-1\n",
    "                    dy = -dy\n",
    "                    \n",
    "                if sx < 0:\n",
    "                    sx = 0 \n",
    "                    dx = -dx\n",
    "                elif sx >= image_size-32:\n",
    "                    sx = image_size-32-1\n",
    "                    dx = -dx\n",
    "                   \n",
    "                x[t, sy:sy+32, sx:sx+32, n] = np.copy(digit.numpy())\n",
    "                sy += dy\n",
    "                sx += dx\n",
    "        # pick on digit to be in front\n",
    "        front = np.random.randint(self.num_digits)\n",
    "        for cc in range(self.num_digits):\n",
    "            if cc != front:\n",
    "                x[:, :, :, cc][x[:, :, :, front] > 0] = 0\n",
    "        return x\n",
    "\n",
    "def sequence_input(seq):\n",
    "    return [Variable(x.type(torch.cuda.FloatTensor)) for x in seq]\n",
    "    \n",
    "def normalize_data(sequence):\n",
    "    sequence.transpose_(0, 1)\n",
    "    sequence.transpose_(3, 4).transpose_(2, 3)\n",
    "\n",
    "    return sequence_input(sequence)\n",
    "\n",
    "def get_training_batch(train_loader):\n",
    "\twhile True:\n",
    "\t\tfor sequence in train_loader:\n",
    "\t\t\tbatch = normalize_data(sequence)\n",
    "\t\t\tyield batch\n",
    "\n",
    "def get_testing_batch(test_loader):\n",
    "\twhile True:\n",
    "\t\tfor sequence in test_loader:\n",
    "\t\t\tbatch = normalize_data(sequence)\n",
    "\t\t\tyield batch\n",
    "\n",
    "def make_rgb_plot(ctx, tgt, pred, epoch=999):\n",
    "    num_ctx_frames= ctx.shape[1]\n",
    "    num_tgt_frames = tgt.shape[1]\n",
    "\n",
    "    def show_frames(frames, ax, row_label=None):\n",
    "        for i, frame in enumerate(frames):\n",
    "            ax[i].imshow(frame)\n",
    "            ax[i].set_xticks([])\n",
    "            ax[i].set_yticks([])\n",
    "\n",
    "        if row_label is not None:\n",
    "            ax[0].set_ylabel(row_label)\n",
    "\n",
    "    ctx_frames = ctx.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "    tgt_frames = tgt.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "    pred_frames = pred.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(3, max(num_ctx_frames, num_tgt_frames),\n",
    "                       figsize = (9, 5))\n",
    "    fig.suptitle(f\"EPOCH {epoch}\", y=0.93)\n",
    "    show_frames(ctx_frames, ax[0], \"Context\")\n",
    "    show_frames(tgt_frames, ax[1], \"Target\")\n",
    "    show_frames(pred_frames, ax[2], \"Prediction\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/kelvinfung/Documents/bounce-digits/data/\"\n",
    "seq_len=10\n",
    "image_width=128\n",
    "batch_size=16\n",
    "\n",
    "train_data = MovingMNIST(\n",
    "            train=True,\n",
    "            data_root=data_root,\n",
    "            seq_len=seq_len,\n",
    "            image_size=image_width,\n",
    "            num_digits=2)\n",
    "test_data = MovingMNIST(\n",
    "        train=False,\n",
    "        data_root=data_root,\n",
    "        seq_len=seq_len,\n",
    "        image_size=image_width,\n",
    "        num_digits=2)\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                        num_workers=4, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, \n",
    "                        drop_last=True, \n",
    "                        pin_memory=True)\n",
    "test_loader = DataLoader(test_data, \n",
    "                        num_workers=4, \n",
    "                        batch_size=16,\n",
    "                        shuffle=False, \n",
    "                         drop_last=True, \n",
    "                         pin_memory=True)\n",
    "\n",
    "train_generator = get_training_batch(train_loader)\n",
    "test_generator = get_testing_batch(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(train_generator)\n",
    "print(len(x))\n",
    "x[0].shape\n",
    "# x: list of tensors of length = seq_len\n",
    "# x[0] tensor of shape: B * C * H * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=3\n",
    "pose_dim=5\n",
    "discriminator_dim=100\n",
    "\n",
    "scene_discriminator = SceneDiscriminator(pose_dim, discriminator_dim).to(\"cuda\")\n",
    "pose_encoder = Encoder(channels, pose_dim).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.cuda.FloatTensor(batch_size, 1)\n",
    "x1 = x[0].to(\"cuda\")  # First frame of all videos in batch: BS x C x H x W\n",
    "x2 = x[1].to(\"cuda\")  # Second frame of all videos in batch\n",
    "h_p1 = pose_encoder(x1)[0].detach()  # Pose of first frames of all videos in a batch: BS x pose_dim x 1 x 1\n",
    "h_p2 = pose_encoder(x2)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(batch_size/2)\n",
    "rp = torch.randperm(half).cuda()\n",
    "h_p2[:half] = h_p2[rp]  # Permute first half of h_p2; allowing frames from different videos to be compared by the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:half] = 1\n",
    "target[half:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"h_p1 shape: {h_p1.shape}\")\n",
    "out = scene_discriminator(h_p1, h_p2)\n",
    "print(f\"scene discriminator out shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.MSELoss()(out, Variable(target))\n",
    "acc =out[:half].gt(0.5).sum() + out[half:].le(0.5).sum()\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PredRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\"batch_size\": 8,\n",
    "        \"total_length\": 10,\n",
    "        \"input_length\": 5,\n",
    "        \"img_width\": 128,\n",
    "        \"patch_size\": 4,\n",
    "        \"img_channel\": 3,\n",
    "        \"scheduled_sampling\": 1,\n",
    "        \"sampling_stop_iter\": 50000,\n",
    "        \"sampling_changing_rate\": 0.00002}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "args = AttrDict(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_sampling(eta, itr):\n",
    "    zeros = np.zeros((args.batch_size,\n",
    "                      args.total_length - args.input_length - 1,\n",
    "                      args.img_width // args.patch_size,\n",
    "                      args.img_width // args.patch_size,\n",
    "                      args.patch_size ** 2 * args.img_channel))\n",
    "    if not args.scheduled_sampling:\n",
    "        return 0.0, zeros\n",
    "\n",
    "    if itr < args.sampling_stop_iter:\n",
    "        eta -= args.sampling_changing_rate\n",
    "    else:\n",
    "        eta = 0.0\n",
    "    random_flip = np.random.random_sample(\n",
    "        (args.batch_size, args.total_length - args.input_length - 1))\n",
    "    true_token = (random_flip < eta)\n",
    "    ones = np.ones((args.img_width // args.patch_size,\n",
    "                    args.img_width // args.patch_size,\n",
    "                    args.patch_size ** 2 * args.img_channel))\n",
    "    zeros = np.zeros((args.img_width // args.patch_size,\n",
    "                      args.img_width // args.patch_size,\n",
    "                      args.patch_size ** 2 * args.img_channel))\n",
    "    real_input_flag = []\n",
    "    for i in range(args.batch_size):\n",
    "        for j in range(args.total_length - args.input_length - 1):\n",
    "            if true_token[i, j]:\n",
    "                real_input_flag.append(ones)\n",
    "            else:\n",
    "                real_input_flag.append(zeros)\n",
    "    real_input_flag = np.array(real_input_flag)\n",
    "    real_input_flag = np.reshape(real_input_flag,\n",
    "                                 (args.batch_size,\n",
    "                                  args.total_length - args.input_length - 1,\n",
    "                                  args.img_width // args.patch_size,\n",
    "                                  args.img_width // args.patch_size,\n",
    "                                  args.patch_size ** 2 * args.img_channel))\n",
    "    return eta, real_input_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1.0\n",
    "iter = 2\n",
    "\n",
    "eta, real_input_flag = schedule_sampling(eta, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpatioTemporalLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channel, num_hidden, kernel_size, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "        self.padding = kernel_size // 2\n",
    "        self._forget_bias = 1.0\n",
    "\n",
    "        self.conv_x = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, num_hidden * 7, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=False),\n",
    "        )\n",
    "        self.conv_h = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, num_hidden * 4, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=False),\n",
    "        )\n",
    "        self.conv_m = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, num_hidden * 3, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=False),\n",
    "        )\n",
    "        self.conv_o = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=False),\n",
    "        )\n",
    "\n",
    "        self.conv_last = nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "    \n",
    "    def forward(self, x_t, h_t, c_t, m_t):\n",
    "        x_concat = self.conv_x(x_t)\n",
    "        h_concat = self.conv_h(h_t)\n",
    "        m_concat = self.conv_m(m_t)\n",
    "        i_x, f_x, g_x, i_x_prime, f_x_prime, g_x_prime, o_x = torch.split(x_concat, self.num_hidden, dim=1)\n",
    "        i_h, f_h, g_h, o_h = torch.split(h_concat, self.num_hidden, dim=1)\n",
    "        i_m, f_m, g_m = torch.split(m_concat, self.num_hidden, dim=1)\n",
    "\n",
    "        i_t = torch.sigmoid(i_x + i_h)\n",
    "        f_t = torch.sigmoid(f_x + f_h + self._forget_bias)\n",
    "        g_t = torch.tanh(g_x + g_h)\n",
    "\n",
    "        # print(f\"f_t: {f_t.shape}\")\n",
    "        # print(f\"c_t: {c_t.shape}\")\n",
    "        # print(f\"i_t: {i_t.shape}\")\n",
    "        # print(f\"g_t: {g_t.shape}\")\n",
    "        c_new = f_t * c_t + i_t * g_t\n",
    "\n",
    "        i_t_prime = torch.sigmoid(i_x_prime + i_m)\n",
    "        f_t_prime = torch.sigmoid(f_x_prime + f_m + self._forget_bias)\n",
    "        g_t_prime = torch.tanh(g_x_prime + g_m)\n",
    "\n",
    "        m_new = f_t_prime * m_t + i_t_prime * g_t_prime\n",
    "\n",
    "        mem = torch.cat((c_new, m_new), 1)\n",
    "        o_t = torch.sigmoid(o_x + o_h + self.conv_o(mem))\n",
    "        h_new = o_t * torch.tanh(self.conv_last(mem))\n",
    "\n",
    "        return h_new, c_new, m_new\n",
    "\n",
    "class PredRNN(nn.Module):\n",
    "    def __init__(self, input_channels, img_width, img_height,\n",
    "                 num_layers, num_hidden, \n",
    "                 num_ctx_frames, num_tgt_frames,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        cell_list = []\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.num_ctx_frames = num_ctx_frames\n",
    "        self.num_tgt_frames = num_tgt_frames\n",
    "        self.total_length = num_ctx_frames + num_tgt_frames\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.learning_rate = learning_rate \n",
    "\n",
    "        for i in range(num_layers):\n",
    "            in_channel = self.input_channels if i == 0 else num_hidden[i - 1]\n",
    "            cell_list.append(\n",
    "                SpatioTemporalLSTMCell(in_channel, num_hidden[i], kernel_size, stride)\n",
    "            )\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        self.conv_last = nn.Conv2d(num_hidden[num_layers - 1], \n",
    "                                   self.input_channels,\n",
    "                                   kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, F, H, W = x.shape\n",
    "\n",
    "        next_frames = []\n",
    "        h_t = []\n",
    "        c_t = []\n",
    "\n",
    "        # Initialize hidden states and cell states\n",
    "        for i in range(self.num_layers):\n",
    "            zeros = torch.zeros([B, self.num_hidden[i], H, W])\n",
    "            h_t.append(zeros)\n",
    "            c_t.append(zeros)\n",
    "\n",
    "        # Initialize memory state\n",
    "        memory = torch.zeros([B, self.num_hidden[0], H, W])\n",
    "\n",
    "        for t in range(self.total_length - 1):\n",
    "            print(f\"t: {t}\")\n",
    "            frame = x[:, :, t]\n",
    "            h_t[0], c_t[0], memory = self.cell_list[0](frame, h_t[0], c_t[0], memory)\n",
    "\n",
    "            for i in range(1, self.num_layers):\n",
    "                h_t[i], c_t[i], memory = self.cell_list[i](h_t[i - 1], h_t[i], c_t[i], memory)\n",
    "\n",
    "            x_gen = self.conv_last(h_t[self.num_layers - 1])\n",
    "            next_frames.append(x_gen)\n",
    "\n",
    "        next_frames = torch.stack(next_frames, dim=2)\n",
    "        return next_frames\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, )\n",
    "        return {\"optimizer\": optimizer, \n",
    "                \"lr_scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels=3\n",
    "img_width=128\n",
    "img_height=128\n",
    "num_hidden = [64,64,64]\n",
    "num_layers = len(num_hidden)\n",
    "num_ctx_frames=5\n",
    "num_tgt_frames=5\n",
    "patch_size=4\n",
    "kernel_size=5\n",
    "stride=1\n",
    "layer_norm=0\n",
    "\n",
    "model = PredRNN(input_channels=input_channels, \n",
    "                img_width=img_width, \n",
    "                img_height=img_height,\n",
    "                num_layers=num_layers,\n",
    "                num_hidden=num_hidden, \n",
    "                num_ctx_frames=num_ctx_frames, \n",
    "                num_tgt_frames=num_tgt_frames,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0\n",
      "t: 1\n",
      "t: 2\n",
      "t: 3\n",
      "t: 4\n",
      "t: 5\n",
      "t: 6\n",
      "t: 7\n",
      "t: 8\n"
     ]
    }
   ],
   "source": [
    "train_x_batch = torch.randn(8, 3, 10, 128, 128)\n",
    "next_frames = model(train_x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 9, 128, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4c3a4107fc661dfc1ddc51b98664f856b9baf685ab1745d9fa2472938977d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
