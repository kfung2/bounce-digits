{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/kelvinfung/Documents/bounce-digits\")\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, F, H, W = 50, 1, 5, 64, 64\n",
    "vid_sample = torch.randn(N, C, F, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without stride; same output HxW\n",
    "conv3d = nn.Conv3d(1, 5,\n",
    "                   stride=(1, 1, 1),\n",
    "                   kernel_size=(3,3,3), padding=(1,1,1))\n",
    "\n",
    "conv3d(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With stride; Downsample by halving height and width\n",
    "conv3d = nn.Conv3d(1, 5,\n",
    "                   stride=(1, 2, 2),\n",
    "                   kernel_size=(3,3,3), padding=(1,1,1))\n",
    "\n",
    "conv3d(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranpose; Same height and width\n",
    "conv3dtranspose = nn.ConvTranspose3d(1, 5,\n",
    "                           stride=(1,1,1),kernel_size=(3,3,3), \n",
    "                           padding=(1,1,1))\n",
    "\n",
    "conv3dtranspose(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranpose upsample; Doubles the height and width\n",
    "conv3dtranspose = nn.ConvTranspose3d(1, 5,\n",
    "                           stride=(1,2,2),kernel_size=(3,3,3), \n",
    "                           padding=(1,1,1), output_padding=(0,1,1))\n",
    "\n",
    "conv3dtranspose(vid_sample).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample batch of context frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, F, H, W = 50, 1, 5, 64, 64\n",
    "sample_batch = torch.randn(N, C, F, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = torch.randint_like(vid_sample, low=-5, high=15)\n",
    "plt.hist(sample_batch.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FutureDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'nframes_pred': 5,\n",
    "#     'nframes_in' : 5,\n",
    "#     'batch_norm' : False,\n",
    "#     'w_norm' : True,\n",
    "#     'loss' : 'wgan_gp',\n",
    "#     'd_gdrop' : False,\n",
    "#     'padding' : 'zero',\n",
    "#     'lrelu' : True,\n",
    "#     'd_sigmoid' : False,\n",
    "#     'nz' : 512,            # dim of input noise vector z    \n",
    "#     'nc' : 1,              # number of channels\n",
    "#     'ndf' : 512,           # discriminator first layer's feature dim\n",
    "#     'd_cond' : True\n",
    "# }\n",
    "\n",
    "# dis = Discriminator(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "######## Refer to the implementation given at 'https://github.com/edenton/drnet-py' for indepth understanding ###########\n",
    "\n",
    "class MovingMNIST(object):\n",
    "    \n",
    "    \"\"\"Data Handler that creates Bouncing MNIST dataset on the fly.\"\"\"\n",
    "\n",
    "    def __init__(self, train, data_root, seq_len=20, num_digits=2, image_size=64):\n",
    "        path = data_root\n",
    "        self.seq_len = seq_len\n",
    "        self.num_digits = num_digits  \n",
    "        self.image_size = image_size \n",
    "        self.step_length = 0.1\n",
    "        self.digit_size = 32\n",
    "        self.seed_is_set = False # multi threaded loading\n",
    "\n",
    "        self.data = datasets.MNIST(\n",
    "            path,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.Resize(self.digit_size),\n",
    "                 transforms.ToTensor()]))\n",
    "\n",
    "        self.N = len(self.data) \n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        if not self.seed_is_set:\n",
    "            self.seed_is_set = True\n",
    "            np.random.seed(seed)\n",
    "          \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.set_seed(index)\n",
    "        image_size = self.image_size\n",
    "        digit_size = self.digit_size\n",
    "        x = np.zeros((self.seq_len,\n",
    "                      image_size, \n",
    "                      image_size, \n",
    "                      3),\n",
    "                    dtype=np.float32)\n",
    "        for n in range(self.num_digits):\n",
    "            idx = np.random.randint(self.N)\n",
    "            digit, _ = self.data[idx]\n",
    "\n",
    "            sx = np.random.randint(image_size-digit_size)\n",
    "            sy = np.random.randint(image_size-digit_size)\n",
    "            dx = np.random.randint(-4, 4)\n",
    "            dy = np.random.randint(-4, 4)\n",
    "            for t in range(self.seq_len):\n",
    "                if sy < 0:\n",
    "                    sy = 0 \n",
    "                    dy = -dy\n",
    "                elif sy >= image_size-32:\n",
    "                    sy = image_size-32-1\n",
    "                    dy = -dy\n",
    "                    \n",
    "                if sx < 0:\n",
    "                    sx = 0 \n",
    "                    dx = -dx\n",
    "                elif sx >= image_size-32:\n",
    "                    sx = image_size-32-1\n",
    "                    dx = -dx\n",
    "                   \n",
    "                x[t, sy:sy+32, sx:sx+32, n] = np.copy(digit.numpy())\n",
    "                sy += dy\n",
    "                sx += dx\n",
    "        # pick on digit to be in front\n",
    "        front = np.random.randint(self.num_digits)\n",
    "        for cc in range(self.num_digits):\n",
    "            if cc != front:\n",
    "                x[:, :, :, cc][x[:, :, :, front] > 0] = 0\n",
    "        return x\n",
    "\n",
    "def sequence_input(seq):\n",
    "    return [Variable(x.type(torch.cuda.FloatTensor)) for x in seq]\n",
    "    \n",
    "def normalize_data(sequence):\n",
    "    sequence.transpose_(0, 1)\n",
    "    sequence.transpose_(3, 4).transpose_(2, 3)\n",
    "\n",
    "    return sequence_input(sequence)\n",
    "\n",
    "def get_training_batch(train_loader):\n",
    "\twhile True:\n",
    "\t\tfor sequence in train_loader:\n",
    "\t\t\tbatch = normalize_data(sequence)\n",
    "\t\t\tyield batch\n",
    "\n",
    "def get_testing_batch(test_loader):\n",
    "\twhile True:\n",
    "\t\tfor sequence in test_loader:\n",
    "\t\t\tbatch = normalize_data(sequence)\n",
    "\t\t\tyield batch\n",
    "\n",
    "def make_rgb_plot(ctx, tgt, pred, epoch=999):\n",
    "    num_ctx_frames= ctx.shape[1]\n",
    "    num_tgt_frames = tgt.shape[1]\n",
    "\n",
    "    def show_frames(frames, ax, row_label=None):\n",
    "        for i, frame in enumerate(frames):\n",
    "            ax[i].imshow(frame)\n",
    "            ax[i].set_xticks([])\n",
    "            ax[i].set_yticks([])\n",
    "\n",
    "        if row_label is not None:\n",
    "            ax[0].set_ylabel(row_label)\n",
    "\n",
    "    ctx_frames = ctx.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "    tgt_frames = tgt.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "    pred_frames = pred.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(3, max(num_ctx_frames, num_tgt_frames),\n",
    "                       figsize = (9, 5))\n",
    "    fig.suptitle(f\"EPOCH {epoch}\", y=0.93)\n",
    "    show_frames(ctx_frames, ax[0], \"Context\")\n",
    "    show_frames(tgt_frames, ax[1], \"Target\")\n",
    "    show_frames(pred_frames, ax[2], \"Prediction\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/kelvinfung/Documents/bounce-digits/data/\"\n",
    "seq_len=10\n",
    "image_width=128\n",
    "batch_size=16\n",
    "\n",
    "train_data = MovingMNIST(\n",
    "            train=True,\n",
    "            data_root=data_root,\n",
    "            seq_len=seq_len,\n",
    "            image_size=image_width,\n",
    "            num_digits=2)\n",
    "test_data = MovingMNIST(\n",
    "        train=False,\n",
    "        data_root=data_root,\n",
    "        seq_len=seq_len,\n",
    "        image_size=image_width,\n",
    "        num_digits=2)\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                        num_workers=4, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, \n",
    "                        drop_last=True, \n",
    "                        pin_memory=True)\n",
    "test_loader = DataLoader(test_data, \n",
    "                        num_workers=4, \n",
    "                        batch_size=16,\n",
    "                        shuffle=False, \n",
    "                         drop_last=True, \n",
    "                         pin_memory=True)\n",
    "\n",
    "train_generator = get_training_batch(train_loader)\n",
    "test_generator = get_testing_batch(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(train_generator)\n",
    "print(len(x))\n",
    "x[0].shape\n",
    "# x: list of tensors of length = seq_len\n",
    "# x[0] tensor of shape: B * C * H * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=3\n",
    "pose_dim=5\n",
    "discriminator_dim=100\n",
    "\n",
    "scene_discriminator = SceneDiscriminator(pose_dim, discriminator_dim).to(\"cuda\")\n",
    "pose_encoder = Encoder(channels, pose_dim).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.cuda.FloatTensor(batch_size, 1)\n",
    "x1 = x[0].to(\"cuda\")  # First frame of all videos in batch: BS x C x H x W\n",
    "x2 = x[1].to(\"cuda\")  # Second frame of all videos in batch\n",
    "h_p1 = pose_encoder(x1)[0].detach()  # Pose of first frames of all videos in a batch: BS x pose_dim x 1 x 1\n",
    "h_p2 = pose_encoder(x2)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(batch_size/2)\n",
    "rp = torch.randperm(half).cuda()\n",
    "h_p2[:half] = h_p2[rp]  # Permute first half of h_p2; allowing frames from different videos to be compared by the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:half] = 1\n",
    "target[half:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"h_p1 shape: {h_p1.shape}\")\n",
    "out = scene_discriminator(h_p1, h_p2)\n",
    "print(f\"scene discriminator out shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.MSELoss()(out, Variable(target))\n",
    "acc =out[:half].gt(0.5).sum() + out[half:].le(0.5).sum()\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.FloatTensor(batch_size, 1).fill_(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = DRNetMain(channels=3, content_dim=128, pose_dim=5,\n",
    "                 discriminator_dim=100,\n",
    "                 learning_rate=1e-3,\n",
    "                 alpha=1,\n",
    "                 beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1271)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce = nn.BCELoss()\n",
    "pred = torch.tensor([0.9, 0.8, 0.9, 0.1, 0, 0.2])\n",
    "target = torch.tensor([1., 1., 1., 0., 0., 0.])\n",
    "bce(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4c3a4107fc661dfc1ddc51b98664f856b9baf685ab1745d9fa2472938977d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
