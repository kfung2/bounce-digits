{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/kelvinfung/Documents/bounce-digits\")\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, F, H, W = 50, 1, 5, 64, 64\n",
    "vid_sample = torch.randn(N, C, F, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without stride; same output HxW\n",
    "conv3d = nn.Conv3d(1, 5,\n",
    "                   stride=(1, 1, 1),\n",
    "                   kernel_size=(3,3,3), padding=(1,1,1))\n",
    "\n",
    "conv3d(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With stride; Downsample by halving height and width\n",
    "conv3d = nn.Conv3d(1, 5,\n",
    "                   stride=(1, 2, 2),\n",
    "                   kernel_size=(3,3,3), padding=(1,1,1))\n",
    "\n",
    "conv3d(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranpose; Same height and width\n",
    "conv3dtranspose = nn.ConvTranspose3d(1, 5,\n",
    "                           stride=(1,1,1),kernel_size=(3,3,3), \n",
    "                           padding=(1,1,1))\n",
    "\n",
    "conv3dtranspose(vid_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranpose upsample; Doubles the height and width\n",
    "conv3dtranspose = nn.ConvTranspose3d(1, 5,\n",
    "                           stride=(1,2,2),kernel_size=(3,3,3), \n",
    "                           padding=(1,1,1), output_padding=(0,1,1))\n",
    "\n",
    "conv3dtranspose(vid_sample).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample batch of context frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, F, H, W = 50, 1, 5, 64, 64\n",
    "sample_batch = torch.randn(N, C, F, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = torch.randint_like(vid_sample, low=-5, high=15)\n",
    "plt.hist(sample_batch.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FutureDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'nframes_pred': 5,\n",
    "#     'nframes_in' : 5,\n",
    "#     'batch_norm' : False,\n",
    "#     'w_norm' : True,\n",
    "#     'loss' : 'wgan_gp',\n",
    "#     'd_gdrop' : False,\n",
    "#     'padding' : 'zero',\n",
    "#     'lrelu' : True,\n",
    "#     'd_sigmoid' : False,\n",
    "#     'nz' : 512,            # dim of input noise vector z    \n",
    "#     'nc' : 1,              # number of channels\n",
    "#     'ndf' : 512,           # discriminator first layer's feature dim\n",
    "#     'd_cond' : True\n",
    "# }\n",
    "\n",
    "# dis = Discriminator(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "######## Refer to the implementation given at 'https://github.com/edenton/drnet-py' for indepth understanding ###########\n",
    "\n",
    "class MovingMNIST(object):\n",
    "    \n",
    "    \"\"\"Data Handler that creates Bouncing MNIST dataset on the fly.\"\"\"\n",
    "\n",
    "    def __init__(self, train, data_root, seq_len=20, num_digits=2, image_size=64):\n",
    "        path = data_root\n",
    "        self.seq_len = seq_len\n",
    "        self.num_digits = num_digits  \n",
    "        self.image_size = image_size \n",
    "        self.step_length = 0.1\n",
    "        self.digit_size = 32\n",
    "        self.seed_is_set = False # multi threaded loading\n",
    "\n",
    "        self.data = datasets.MNIST(\n",
    "            path,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.Resize(self.digit_size),\n",
    "                 transforms.ToTensor()]))\n",
    "\n",
    "        self.N = len(self.data) \n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        if not self.seed_is_set:\n",
    "            self.seed_is_set = True\n",
    "            np.random.seed(seed)\n",
    "          \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.set_seed(index)\n",
    "        image_size = self.image_size\n",
    "        digit_size = self.digit_size\n",
    "        x = np.zeros((self.seq_len,\n",
    "                      image_size, \n",
    "                      image_size, \n",
    "                      3),\n",
    "                    dtype=np.float32)\n",
    "        for n in range(self.num_digits):\n",
    "            idx = np.random.randint(self.N)\n",
    "            digit, _ = self.data[idx]\n",
    "\n",
    "            sx = np.random.randint(image_size-digit_size)\n",
    "            sy = np.random.randint(image_size-digit_size)\n",
    "            dx = np.random.randint(-4, 4)\n",
    "            dy = np.random.randint(-4, 4)\n",
    "            for t in range(self.seq_len):\n",
    "                if sy < 0:\n",
    "                    sy = 0 \n",
    "                    dy = -dy\n",
    "                elif sy >= image_size-32:\n",
    "                    sy = image_size-32-1\n",
    "                    dy = -dy\n",
    "                    \n",
    "                if sx < 0:\n",
    "                    sx = 0 \n",
    "                    dx = -dx\n",
    "                elif sx >= image_size-32:\n",
    "                    sx = image_size-32-1\n",
    "                    dx = -dx\n",
    "                   \n",
    "                x[t, sy:sy+32, sx:sx+32, n] = np.copy(digit.numpy())\n",
    "                sy += dy\n",
    "                sx += dx\n",
    "        # pick on digit to be in front\n",
    "        front = np.random.randint(self.num_digits)\n",
    "        for cc in range(self.num_digits):\n",
    "            if cc != front:\n",
    "                x[:, :, :, cc][x[:, :, :, front] > 0] = 0\n",
    "        return x\n",
    "\n",
    "def sequence_input(seq):\n",
    "    return [Variable(x.type(torch.cuda.FloatTensor)) for x in seq]\n",
    "    \n",
    "def normalize_data(sequence):\n",
    "    sequence.transpose_(0, 1)\n",
    "    sequence.transpose_(3, 4).transpose_(2, 3)\n",
    "\n",
    "    return sequence_input(sequence)\n",
    "\n",
    "def get_training_batch(train_loader):\n",
    "\twhile True:\n",
    "\t\tfor sequence in train_loader:\n",
    "\t\t\tbatch = normalize_data(sequence)\n",
    "\t\t\tyield batch\n",
    "\n",
    "def get_testing_batch(test_loader):\n",
    "\twhile True:\n",
    "\t\tfor sequence in test_loader:\n",
    "\t\t\tbatch = normalize_data(sequence)\n",
    "\t\t\tyield batch\n",
    "\n",
    "def make_rgb_plot(ctx, tgt, pred, epoch=999):\n",
    "    num_ctx_frames= ctx.shape[1]\n",
    "    num_tgt_frames = tgt.shape[1]\n",
    "\n",
    "    def show_frames(frames, ax, row_label=None):\n",
    "        for i, frame in enumerate(frames):\n",
    "            ax[i].imshow(frame)\n",
    "            ax[i].set_xticks([])\n",
    "            ax[i].set_yticks([])\n",
    "\n",
    "        if row_label is not None:\n",
    "            ax[0].set_ylabel(row_label)\n",
    "\n",
    "    ctx_frames = ctx.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "    tgt_frames = tgt.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "    pred_frames = pred.squeeze().permute(1, 2, 3, 0).cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(3, max(num_ctx_frames, num_tgt_frames),\n",
    "                       figsize = (9, 5))\n",
    "    fig.suptitle(f\"EPOCH {epoch}\", y=0.93)\n",
    "    show_frames(ctx_frames, ax[0], \"Context\")\n",
    "    show_frames(tgt_frames, ax[1], \"Target\")\n",
    "    show_frames(pred_frames, ax[2], \"Prediction\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/kelvinfung/Documents/bounce-digits/data/\"\n",
    "seq_len=10\n",
    "image_width=128\n",
    "batch_size=16\n",
    "\n",
    "train_data = MovingMNIST(\n",
    "            train=True,\n",
    "            data_root=data_root,\n",
    "            seq_len=seq_len,\n",
    "            image_size=image_width,\n",
    "            num_digits=2)\n",
    "test_data = MovingMNIST(\n",
    "        train=False,\n",
    "        data_root=data_root,\n",
    "        seq_len=seq_len,\n",
    "        image_size=image_width,\n",
    "        num_digits=2)\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                        num_workers=4, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, \n",
    "                        drop_last=True, \n",
    "                        pin_memory=True)\n",
    "test_loader = DataLoader(test_data, \n",
    "                        num_workers=4, \n",
    "                        batch_size=16,\n",
    "                        shuffle=False, \n",
    "                         drop_last=True, \n",
    "                         pin_memory=True)\n",
    "\n",
    "train_generator = get_training_batch(train_loader)\n",
    "test_generator = get_testing_batch(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(train_generator)\n",
    "print(len(x))\n",
    "x[0].shape\n",
    "# x: list of tensors of length = seq_len\n",
    "# x[0] tensor of shape: B * C * H * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=3\n",
    "pose_dim=5\n",
    "discriminator_dim=100\n",
    "\n",
    "scene_discriminator = SceneDiscriminator(pose_dim, discriminator_dim).to(\"cuda\")\n",
    "pose_encoder = Encoder(channels, pose_dim).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.cuda.FloatTensor(batch_size, 1)\n",
    "x1 = x[0].to(\"cuda\")  # First frame of all videos in batch: BS x C x H x W\n",
    "x2 = x[1].to(\"cuda\")  # Second frame of all videos in batch\n",
    "h_p1 = pose_encoder(x1)[0].detach()  # Pose of first frames of all videos in a batch: BS x pose_dim x 1 x 1\n",
    "h_p2 = pose_encoder(x2)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(batch_size/2)\n",
    "rp = torch.randperm(half).cuda()\n",
    "h_p2[:half] = h_p2[rp]  # Permute first half of h_p2; allowing frames from different videos to be compared by the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:half] = 1\n",
    "target[half:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"h_p1 shape: {h_p1.shape}\")\n",
    "out = scene_discriminator(h_p1, h_p2)\n",
    "print(f\"scene discriminator out shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.MSELoss()(out, Variable(target))\n",
    "acc =out[:half].gt(0.5).sum() + out[half:].le(0.5).sum()\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=input_dim + hidden_dim,\n",
    "                              out_channels=4 * hidden_dim,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=bias)\n",
    "\n",
    "    def forward(self, input_tensor, h_curr, c_curr):\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_curr], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)  # Inconsistent with ConvLSTM equations on blog\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_curr + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size, bias,\n",
    "                 num_tgt_frames=5,\n",
    "                 learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.mse = nn.MSELoss()\n",
    "        # self.ssim = SSIM()\n",
    "        # self.psnr = PSNR()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_tgt_frames = num_tgt_frames\n",
    "\n",
    "        self.encoder_1 = ConvLSTMCell(input_dim=input_dim,\n",
    "                                      hidden_dim=hidden_dim,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      bias=bias)\n",
    "    \n",
    "        self.encoder_2 = ConvLSTMCell(input_dim=hidden_dim,\n",
    "                                  hidden_dim=hidden_dim,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  bias=bias)\n",
    "\n",
    "        self.decoder_1 = ConvLSTMCell(input_dim=hidden_dim,\n",
    "                                  hidden_dim=hidden_dim,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  bias=bias)\n",
    "        \n",
    "        self.decoder_2 = ConvLSTMCell(input_dim=hidden_dim,\n",
    "                                  hidden_dim=hidden_dim,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  bias=bias)\n",
    "        \n",
    "        self.decoder_CNN = nn.Conv3d(in_channels=hidden_dim,\n",
    "                                     out_channels=output_dim,\n",
    "                                     kernel_size=(1, 3, 3),\n",
    "                                     padding=(0, 1, 1))\n",
    "        \n",
    "    \n",
    "    def autoencoder(self, x, num_ctx_frames, num_tgt_frames, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # encoder\n",
    "        for t in range(num_ctx_frames):\n",
    "            h_t, c_t = self.encoder_1(input_tensor=x[:, :, t],\n",
    "                                      h_curr=h_t, c_curr=c_t)\n",
    "            h_t2, c_t2 = self.encoder_2(input_tensor=h_t,\n",
    "                                        h_curr=h_t2, c_curr=c_t2)  \n",
    "\n",
    "        # encoder_vector\n",
    "        encoder_vector = h_t2\n",
    "        print(f\"encoder_vector: {encoder_vector.shape}\")\n",
    "\n",
    "        # decoder\n",
    "        for t in range(num_tgt_frames):\n",
    "            h_t3, c_t3 = self.decoder_1(input_tensor=encoder_vector,\n",
    "                                        h_curr=h_t3, c_curr=c_t3) \n",
    "            h_t4, c_t4 = self.decoder_2(input_tensor=h_t3,\n",
    "                                        h_curr=h_t4, c_curr=c_t4)  \n",
    "            print(f\"h_t4: {h_t4.shape}\")\n",
    "            outputs += [h_t4]  # predictions\n",
    "\n",
    "        outputs = torch.stack(outputs, 1)\n",
    "        outputs = outputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = self.decoder_CNN(outputs)\n",
    "        outputs = torch.nn.Sigmoid()(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, num_ctx_frames, H, W = x.shape\n",
    "\n",
    "        # initialize hidden states\n",
    "        h_t, c_t = self.encoder_1.init_hidden(batch_size=B, image_size=(H, W))\n",
    "        h_t2, c_t2 = self.encoder_2.init_hidden(batch_size=B, image_size=(H, W))\n",
    "        h_t3, c_t3 = self.decoder_1.init_hidden(batch_size=B, image_size=(H, W))\n",
    "        h_t4, c_t4 = self.decoder_2.init_hidden(batch_size=B, image_size=(H, W))\n",
    "\n",
    "        # autoencoder forward\n",
    "        outputs = self.autoencoder(x, num_ctx_frames, self.num_tgt_frames, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "hidden_dim=64\n",
    "output_dim=3\n",
    "kernel_size=(3, 3)\n",
    "bias = True\n",
    "\n",
    "model = EncoderDecoderConvLSTM(input_dim, hidden_dim, output_dim, \n",
    "                               kernel_size=kernel_size,\n",
    "                               bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 5, 128, 128])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_frames = torch.rand(16, 3, 5, 128, 128)\n",
    "ctx_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_vector: torch.Size([16, 64, 128, 128])\n",
      "h_t4: torch.Size([16, 64, 128, 128])\n",
      "h_t4: torch.Size([16, 64, 128, 128])\n",
      "h_t4: torch.Size([16, 64, 128, 128])\n",
      "h_t4: torch.Size([16, 64, 128, 128])\n",
      "h_t4: torch.Size([16, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(ctx_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 5, 128, 128])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 64, 128, 128])\n",
      "torch.Size([16, 64, 4, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for i in range(4):\n",
    "    outputs += [torch.rand(16, 64, 128, 128)]\n",
    "outputs = torch.stack(outputs, 1)\n",
    "print(outputs.shape)\n",
    "outputs = outputs.permute(0, 2, 1, 3, 4)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 4, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "dec = nn.Conv3d(in_channels=hidden_dim,\n",
    "                                     out_channels=output_dim,\n",
    "                                     kernel_size=(1, 3, 3),\n",
    "                                     padding=(0, 1, 1))\n",
    "\n",
    "print(dec(outputs).shape)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da4c3a4107fc661dfc1ddc51b98664f856b9baf685ab1745d9fa2472938977d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
